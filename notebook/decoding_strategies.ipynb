{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mike/envs/pytorch_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 4548885/4548885 [00:01<00:00, 3424710.22 examples/s]\n",
      "Generating validation split: 100%|██████████| 2169/2169 [00:00<00:00, 1095285.98 examples/s]\n",
      "Generating test split: 100%|██████████| 2999/2999 [00:00<00:00, 1463662.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Die von den Ermittlern an beiden Enden des Staates veröffentlichten Details, wie auch das, was Studenten und Mitarbeiter, die ihn kannten, aussagten, half dabei, ein Bild von einem talentierten, aber möglicherweise schwierigen Lehrer zu zeichnen.\n",
      "\n",
      "Target Text:\n",
      "The details released by investigators at both ends of the state as well as students and staff who knew him helped paint a picture of a talented but possibly troubled teacher.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Im März hatte Netanyahu auf Einladung der Republikaner vor dem US-Kongress eine umstrittene Rede gehalten, die teils als Affront gegen Obama gewertet wurde.\n",
      "Original translation: \n",
      "In March, at the invitation of the Republicans, Netanyahu made a controversial speech to the US Congress, which was partly seen as an affront to Obama.\n",
      "Greedy Search: \n",
      "In March, Netanyahu was invited to the US Congress to participate in a re-election campaign, which was part of the Obama campaign.\n",
      "Beam Search: \n",
      "In March, Netanyahu was invited to the US Congress to represent the US Congress, which was part of the Obama campaign.\n",
      "Temperature Sampling: \n",
      "In March, Netanyahu held the president's invitation to the US Congress, which included part of the White House voting against Obama.\n",
      "Top-k Sampling: \n",
      "In March, Netanyahu had been presented with an invitation of the former congress members to the State Board of Representatives by the US Congress. In March, the re-elections were held among the Members of the House.\n",
      "Top-p Sampling: \n",
      "In March, Netanyahu signed the signing of an invitation to the US Congress to resign, to which he gave up his presidency.\n",
      "Contrastive Search: \n",
      "In March, Netanyahu was invited to the US Congress to participate in a presidential election, which was a victory for Obama.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    ">>> Yes, deterministic methods tend to produce more coherent translations, but sometimes might get stuck repeating the same words, while the stochastic ones, result in more diverse outputs, causing sometimes to hallucinate. The contrastive search appears to be a balance between both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 8.15kB [00:00, 2.63MB/s]\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 11.37\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 12.30\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 14.38\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for contrastive: 19.34\n"
     ]
    }
   ],
   "source": [
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p', 'contrastive']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "new_hyperparameters = {\n",
    "    'num_beams': 3,\n",
    "    'temperature': 0.6,\n",
    "    'top_k': 10,\n",
    "    'top_p': 0.8,\n",
    "    'penalty_alpha': 0.4\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning code\n",
    "best_results = {}\n",
    "\n",
    "def get_hyperparameter_grid(strategy):\n",
    "    if strategy == 'greedy':\n",
    "        return [{}]  # No tiene hiperparámetros\n",
    "    elif strategy == 'beam':\n",
    "        return [{'num_beams': n, 'length_penalty': lp} \n",
    "                for n in [3,5,7] for lp in [0.8,1.0,1.2]]\n",
    "    elif strategy == 'temperature':\n",
    "        return [{'temperature': t} for t in [0.6,0.7,0.8,1.0,1.2]]\n",
    "    elif strategy == 'top-k':\n",
    "        return [{'top_k': k} for k in [10,20,50,100]]\n",
    "    elif strategy == 'top-p':\n",
    "        return [{'top_p': p} for p in [0.8,0.9,0.95]]\n",
    "    elif strategy == 'contrastive':\n",
    "        return [{'top_k': k, 'penalty_alpha': a} \n",
    "                for k in [3,5,7] for a in [0.4,0.6,0.7]]\n",
    "\n",
    "for strategy in strategies:\n",
    "    best_bleu = 0\n",
    "    best_params = {}\n",
    "    param_grid = get_hyperparameter_grid(strategy)  \n",
    "\n",
    "    for params in param_grid:\n",
    "        predictions, references = [], []\n",
    "        for src, tgt in zip(dataset_nmt_preproc['src_texts'], dataset_nmt_preproc['tgt_texts']):\n",
    "            pred = generate_text(src, strategy=strategy, **params)\n",
    "            predictions.append(pred)\n",
    "            references.append([tgt])\n",
    "        bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')['score']\n",
    "\n",
    "        if bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            best_params = params\n",
    "\n",
    "    best_results[strategy] = {'BLEU': best_bleu, 'params': best_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 20.98821693903096, 'params': {}}\n",
      "{'BLEU': 21.940264780375937, 'params': {'num_beams': 3, 'length_penalty': 0.8}}\n",
      "{'BLEU': 17.905244283153074, 'params': {'temperature': 0.6}}\n",
      "{'BLEU': 13.402206825495583, 'params': {'top_k': 10}}\n",
      "{'BLEU': 17.41548491202558, 'params': {'top_p': 0.8}}\n",
      "{'BLEU': 20.289416727605808, 'params': {'top_k': 7, 'penalty_alpha': 0.4}}\n"
     ]
    }
   ],
   "source": [
    "for key in best_results.keys():\n",
    "    print(best_results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    ">>> I would definetely choose the deterministic strategies, they show better BLEU Score while producing coherent outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 232360/232360 [00:00<00:00, 2817128.66 examples/s]\n",
      "Generating sample_length_10_to_292 split: 100%|██████████| 1000/1000 [00:00<00:00, 657930.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " Even with all the stars on the sky , the night will always remain dark .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  `` I do n't care about the million dollars you stole , just tell me where the hell my pet snail is ! ''\n",
      "\n",
      "Greedy Search: \n",
      "i'm a huge pet snail , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and i'm a huge slob , and \n",
      "Beam Search: \n",
      " I do n't care about the million dollars you stole , just tell me where the hell my pet snail is !  I do n't care about the million dollars you stole , just tell me where the hell my pet snail is !  I do n't care about the million dollars you stole , just tell me where the hell my pet snail is ! \n",
      "Temperature Sampling: \n",
      "julia 's mom, a beautiful pet snail named tairis , took her to see what he was doing . In a conversation , she asked her :  What is it? ? '' i'm sure it 's a huge gift and it is much better than my own .  I do n't care about the million dollars you stole , just tell me where the hell my pet snail is ! '' i'm afraid i 'm going to get yours . '' p.s.  Dont you know what it is ? i don't care about what , i don't care about what a nice pet snail is .\n",
      "Top-k Sampling: \n",
      "We are remorsing the past month as a whole as my pet snail (on the other hand ) has been having health issues for six years . I'm sad to say but my pet snail has been huling into her life for ten to twenty years . After a month of working in an icky pet seaweed farm owner 's family I would tell them that they have not asked anyone for their cat's medicine , and that they have found the killer snail responsible , since it's really a problem in the seaweed. I am glad they don't think they will need her as they think it would be a problem in the seaweed or the whole world .\n",
      "Top-p Sampling: \n",
      "I just found out my husband stole a million dollars from a pet snail . When he tried to kill him for the money he grabbed , he was so thrilled . The snail, by contrast, is a massive snail and she is really terrible ! But now that that's not exactly the case - she took one of her pets. And there were not a lot of people that would go with her, so it was time to pick that up the money and buy some things she actually ran out of money . '' Anyway , he put it on her .\n",
      "Contrastive Search: \n",
      "i was a little bit of a snob , but i was a bit of a snob . i was a bit of a snob , but i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a snob , and i was a bit of a \n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=7)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.85)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=75)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.98)\n",
    "story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=6, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    ">>> Once more, deterministic strategies produce more rigid text, but repeating itself, and the contrastive search now appears to get stuck also repeating itself. In this case the best option seems to be the stochastic strategies, as the randomness they produce result in more creative and better answers than the other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    ">>> I have used a loop with differente hyperparameters to obtain some optimal ones, which are: num_beams = 3, temperature = 0.6, top_k = 10, top_p = 0.8, and penalty_alpha = 0.4. The BLUE scores show that beam search achieved the highest score (21.94), better greedy decoding (20.99) and contrastive decoding (20.47), while stochastic strategies performed less good. My conclusion is that beam search, is more effective for this first use case, as it balances exploration and exploitation.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    ">>> I tried several hyperparameters and found that Beam Search with num_beams=7 produced repetitive outputs, while stochastic samplings with its respective hyperparameter: temperature (0.85), top-k (75), and top-p (0.98), generated more diverse and creative stories. Contrastive Search with top_k=6 and penalty_alpha=0.6 still had loops. In conclusion, stochastic strategies in this case offer the best balance between creativity and coherence, making them the best ones for generating a story."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
